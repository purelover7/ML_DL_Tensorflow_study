{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 12-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source - Short Sentense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000 loss: 2.317929506 prediction: uuuuuuuuuuu uu \n",
      "0001 loss: 2.209185362 prediction: uuuuuuuuuuu uuu\n",
      "0002 loss: 2.153294086 prediction: u  uu  uuu  uu \n",
      "0003 loss: 2.090982676 prediction: y  oo  ooo  oo \n",
      "0004 loss: 2.016361475 prediction: y  oo  ooo  you\n",
      "0005 loss: 1.925761104 prediction: y  oou ooo  you\n",
      "0006 loss: 1.827335000 prediction: y  you yoo  you\n",
      "0007 loss: 1.745518088 prediction: y  you yoot you\n",
      "0008 loss: 1.660204649 prediction: y  you yott you\n",
      "0009 loss: 1.597371459 prediction: y  you yatt you\n",
      "0010 loss: 1.535698295 prediction: y  you yaot you\n",
      "0011 loss: 1.484851718 prediction: y  you waot you\n",
      "0012 loss: 1.431192160 prediction: y  you watt you\n",
      "0013 loss: 1.403679490 prediction: yf you watt you\n",
      "0014 loss: 1.372415900 prediction: yf you watt you\n",
      "0015 loss: 1.334304094 prediction: yf you waot you\n",
      "0016 loss: 1.310002089 prediction: yf you want you\n",
      "0017 loss: 1.290207148 prediction: yf you want you\n",
      "0018 loss: 1.279789209 prediction: yf you watt you\n",
      "0019 loss: 1.265262127 prediction: yf you want you\n",
      "0020 loss: 1.250233173 prediction: yf you want you\n",
      "0021 loss: 1.237071633 prediction: yf you want you\n",
      "0022 loss: 1.224714279 prediction: yf you want you\n",
      "0023 loss: 1.209196448 prediction: yf you want you\n",
      "0024 loss: 1.191973567 prediction: yf you want you\n",
      "0025 loss: 1.176618814 prediction: yf you want you\n",
      "0026 loss: 1.164772153 prediction: yf you want you\n",
      "0027 loss: 1.156987071 prediction: yf you want you\n",
      "0028 loss: 1.150022507 prediction: yf you want you\n",
      "0029 loss: 1.143837452 prediction: yf you want you\n",
      "0030 loss: 1.138818026 prediction: yf you want you\n",
      "0031 loss: 1.133958459 prediction: if you want you\n",
      "0032 loss: 1.127524137 prediction: if you want you\n",
      "0033 loss: 1.122117400 prediction: if you want you\n",
      "0034 loss: 1.116739988 prediction: if you want you\n",
      "0035 loss: 1.110995770 prediction: if you want you\n",
      "0036 loss: 1.106127501 prediction: if you want you\n",
      "0037 loss: 1.100982785 prediction: if you want you\n",
      "0038 loss: 1.096624255 prediction: if you want you\n",
      "0039 loss: 1.092652202 prediction: if you want you\n",
      "0040 loss: 1.088459730 prediction: if you want you\n",
      "0041 loss: 1.084546447 prediction: if you want you\n",
      "0042 loss: 1.080843687 prediction: if you want you\n",
      "0043 loss: 1.076729894 prediction: if you want you\n",
      "0044 loss: 1.072853565 prediction: if you want you\n",
      "0045 loss: 1.069133282 prediction: if you want you\n",
      "0046 loss: 1.065473318 prediction: if you want you\n",
      "0047 loss: 1.061907411 prediction: if you want you\n",
      "0048 loss: 1.058791280 prediction: if you want you\n",
      "0049 loss: 1.055842638 prediction: if you want you\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sample = \" if you want you\"\n",
    "idx2char = list(set(sample)) # 유니크한 문자열을 리스트로 넣는다.\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)} # 리스트 문자열을 index로 매칭되는 딕셔너리를 만든다.\n",
    "\n",
    "# hyper parameters\n",
    "dic_size = len(char2idx)\n",
    "rnn_hidden_size = len(char2idx)\n",
    "num_classes = len(char2idx)\n",
    "batch_size = 1\n",
    "sequence_length = len(sample)-1\n",
    "\n",
    "sample_idx = [char2idx[c] for c in sample] # char to index\n",
    "x_data = [sample_idx[:-1]] # X data sample (0 ~ n-1) hello:hell\n",
    "y_data = [sample_idx[1:]]  # Y label sample ( 1~ n) hello: ello\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "X_one_hot = tf.one_hot(X, num_classes)\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size,  tf.float32)\n",
    "outputs, _state = tf.nn.dynamic_rnn(cell, X_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)\n",
    "\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        _loss, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "        \n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        # '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost)\n",
    "        print('{0:04d} loss: {1:.9f} prediction: {2}'.format(i, _loss, ''.join(result_str)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source - Long Sentense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  3.21683\n",
      "1 loss:  3.05312\n",
      "2 loss:  2.94064\n",
      "3 loss:  2.87805\n",
      "4 loss:  2.86692\n",
      "5 loss:  2.83854\n",
      "6 loss:  2.82628\n",
      "7 loss:  2.79839\n",
      "8 loss:  2.79745\n",
      "9 loss:  2.75693\n",
      "10 loss:  2.71631\n",
      "11 loss:  2.70143\n",
      "12 loss:  2.67269\n",
      "13 loss:  2.64019\n",
      "14 loss:  2.61806\n",
      "15 loss:  2.59375\n",
      "16 loss:  2.56632\n",
      "17 loss:  2.53713\n",
      "18 loss:  2.51033\n",
      "19 loss:  2.48948\n",
      "20 loss:  2.47062\n",
      "21 loss:  2.44816\n",
      "22 loss:  2.43051\n",
      "23 loss:  2.42116\n",
      "24 loss:  2.40653\n",
      "25 loss:  2.3923\n",
      "26 loss:  2.38458\n",
      "27 loss:  2.37335\n",
      "28 loss:  2.36416\n",
      "29 loss:  2.35399\n",
      "30 loss:  2.34332\n",
      "31 loss:  2.33718\n",
      "32 loss:  2.32653\n",
      "33 loss:  2.32119\n",
      "34 loss:  2.31622\n",
      "35 loss:  2.30851\n",
      "36 loss:  2.30188\n",
      "37 loss:  2.29617\n",
      "38 loss:  2.2907\n",
      "39 loss:  2.28622\n",
      "40 loss:  2.28007\n",
      "41 loss:  2.27604\n",
      "42 loss:  2.27034\n",
      "43 loss:  2.2658\n",
      "44 loss:  2.26042\n",
      "45 loss:  2.25488\n",
      "46 loss:  2.2477\n",
      "47 loss:  2.24124\n",
      "48 loss:  2.23496\n",
      "49 loss:  2.22743\n",
      "50 loss:  2.22071\n",
      "51 loss:  2.21305\n",
      "52 loss:  2.20722\n",
      "53 loss:  2.19933\n",
      "54 loss:  2.19097\n",
      "55 loss:  2.18548\n",
      "56 loss:  2.1824\n",
      "57 loss:  2.17894\n",
      "58 loss:  2.17378\n",
      "59 loss:  2.17127\n",
      "60 loss:  2.16789\n",
      "61 loss:  2.16399\n",
      "62 loss:  2.16011\n",
      "63 loss:  2.15632\n",
      "64 loss:  2.1533\n",
      "65 loss:  2.15174\n",
      "66 loss:  2.14952\n",
      "67 loss:  2.14682\n",
      "68 loss:  2.14553\n",
      "69 loss:  2.14289\n",
      "70 loss:  2.14114\n",
      "71 loss:  2.13916\n",
      "72 loss:  2.13705\n",
      "73 loss:  2.13587\n",
      "74 loss:  2.13432\n",
      "75 loss:  2.13208\n",
      "76 loss:  2.13069\n",
      "77 loss:  2.12902\n",
      "78 loss:  2.12736\n",
      "79 loss:  2.12544\n",
      "80 loss:  2.12458\n",
      "81 loss:  2.12318\n",
      "82 loss:  2.12147\n",
      "83 loss:  2.12011\n",
      "84 loss:  2.11891\n",
      "85 loss:  2.11786\n",
      "86 loss:  2.11629\n",
      "87 loss:  2.11514\n",
      "88 loss:  2.11409\n",
      "89 loss:  2.11257\n",
      "90 loss:  2.11055\n",
      "91 loss:  2.1102\n",
      "92 loss:  2.10842\n",
      "93 loss:  2.10774\n",
      "94 loss:  2.10541\n",
      "95 loss:  2.10329\n",
      "96 loss:  2.10261\n",
      "97 loss:  2.09815\n",
      "98 loss:  2.10067\n",
      "99 loss:  2.09551\n",
      "100 loss:  2.09544\n",
      "101 loss:  2.09324\n",
      "102 loss:  2.09061\n",
      "103 loss:  2.09017\n",
      "104 loss:  2.0909\n",
      "105 loss:  2.08499\n",
      "106 loss:  2.08416\n",
      "107 loss:  2.08271\n",
      "108 loss:  2.07345\n",
      "109 loss:  2.07568\n",
      "110 loss:  2.06989\n",
      "111 loss:  2.06332\n",
      "112 loss:  2.06389\n",
      "113 loss:  2.05809\n",
      "114 loss:  2.05568\n",
      "115 loss:  2.05208\n",
      "116 loss:  2.05047\n",
      "117 loss:  2.04891\n",
      "118 loss:  2.04536\n",
      "119 loss:  2.04446\n",
      "120 loss:  2.04334\n",
      "121 loss:  2.04137\n",
      "122 loss:  2.04039\n",
      "123 loss:  2.03959\n",
      "124 loss:  2.03752\n",
      "125 loss:  2.03622\n",
      "126 loss:  2.03367\n",
      "127 loss:  2.03376\n",
      "128 loss:  2.03183\n",
      "129 loss:  2.03041\n",
      "130 loss:  2.02913\n",
      "131 loss:  2.02845\n",
      "132 loss:  2.02802\n",
      "133 loss:  2.02639\n",
      "134 loss:  2.02514\n",
      "135 loss:  2.02535\n",
      "136 loss:  2.0242\n",
      "137 loss:  2.02328\n",
      "138 loss:  2.02196\n",
      "139 loss:  2.02046\n",
      "140 loss:  2.01987\n",
      "141 loss:  2.01913\n",
      "142 loss:  2.01748\n",
      "143 loss:  2.01672\n",
      "144 loss:  2.01662\n",
      "145 loss:  2.01511\n",
      "146 loss:  2.01435\n",
      "147 loss:  2.01435\n",
      "148 loss:  2.01241\n",
      "149 loss:  2.01167\n",
      "150 loss:  2.01145\n",
      "151 loss:  2.01008\n",
      "152 loss:  2.00964\n",
      "153 loss:  2.01139\n",
      "154 loss:  2.00831\n",
      "155 loss:  2.01047\n",
      "156 loss:  2.01172\n",
      "157 loss:  2.00972\n",
      "158 loss:  2.00819\n",
      "159 loss:  2.00626\n",
      "160 loss:  2.00675\n",
      "161 loss:  2.00501\n",
      "162 loss:  2.0051\n",
      "163 loss:  2.00329\n",
      "164 loss:  2.00378\n",
      "165 loss:  2.00278\n",
      "166 loss:  2.00152\n",
      "167 loss:  2.00094\n",
      "168 loss:  2.00022\n",
      "169 loss:  2.00046\n",
      "170 loss:  1.99987\n",
      "171 loss:  1.99769\n",
      "172 loss:  2.00049\n",
      "173 loss:  1.99973\n",
      "174 loss:  2.00212\n",
      "175 loss:  2.00084\n",
      "176 loss:  1.9986\n",
      "177 loss:  1.99732\n",
      "178 loss:  1.99831\n",
      "179 loss:  1.99945\n",
      "180 loss:  2.00057\n",
      "181 loss:  1.998\n",
      "182 loss:  1.99636\n",
      "183 loss:  1.99943\n",
      "184 loss:  1.99722\n",
      "185 loss:  1.99514\n",
      "186 loss:  1.996\n",
      "187 loss:  1.99158\n",
      "188 loss:  1.99545\n",
      "189 loss:  1.99317\n",
      "190 loss:  1.99542\n",
      "191 loss:  2.00069\n",
      "192 loss:  2.00013\n",
      "193 loss:  2.00224\n",
      "194 loss:  1.99737\n",
      "195 loss:  1.99603\n",
      "196 loss:  1.99595\n",
      "197 loss:  1.99404\n",
      "198 loss:  1.99311\n",
      "199 loss:  1.99064\n",
      "200 loss:  1.98946\n",
      "201 loss:  1.9879\n",
      "202 loss:  1.98672\n",
      "203 loss:  1.98625\n",
      "204 loss:  1.98436\n",
      "205 loss:  1.98259\n",
      "206 loss:  1.98189\n",
      "207 loss:  1.98031\n",
      "208 loss:  1.97948\n",
      "209 loss:  1.97848\n",
      "210 loss:  1.97807\n",
      "211 loss:  1.9774\n",
      "212 loss:  1.9766\n",
      "213 loss:  1.97598\n",
      "214 loss:  1.97513\n",
      "215 loss:  1.97451\n",
      "216 loss:  1.97389\n",
      "217 loss:  1.9731\n",
      "218 loss:  1.9726\n",
      "219 loss:  1.97195\n",
      "220 loss:  1.97099\n",
      "221 loss:  1.9704\n",
      "222 loss:  1.9698\n",
      "223 loss:  1.9692\n",
      "224 loss:  1.9686\n",
      "225 loss:  1.96801\n",
      "226 loss:  1.96741\n",
      "227 loss:  1.96692\n",
      "228 loss:  1.96625\n",
      "229 loss:  1.96573\n",
      "230 loss:  1.96526\n",
      "231 loss:  1.96504\n",
      "232 loss:  1.96652\n",
      "233 loss:  1.96441\n",
      "234 loss:  1.96496\n",
      "235 loss:  1.96489\n",
      "236 loss:  1.96418\n",
      "237 loss:  1.96583\n",
      "238 loss:  1.96985\n",
      "239 loss:  1.97078\n",
      "240 loss:  1.96704\n",
      "241 loss:  1.96696\n",
      "242 loss:  1.96562\n",
      "243 loss:  1.9688\n",
      "244 loss:  1.96761\n",
      "245 loss:  1.96786\n",
      "246 loss:  1.96751\n",
      "247 loss:  1.9661\n",
      "248 loss:  1.96478\n",
      "249 loss:  1.96296\n",
      "250 loss:  1.96211\n",
      "251 loss:  1.96213\n",
      "252 loss:  1.96172\n",
      "253 loss:  1.96145\n",
      "254 loss:  1.96\n",
      "255 loss:  1.95918\n",
      "256 loss:  1.9581\n",
      "257 loss:  1.95815\n",
      "258 loss:  1.95791\n",
      "259 loss:  1.95753\n",
      "260 loss:  1.95689\n",
      "261 loss:  1.95663\n",
      "262 loss:  1.95621\n",
      "263 loss:  1.95544\n",
      "264 loss:  1.95589\n",
      "265 loss:  1.95515\n",
      "266 loss:  1.95446\n",
      "267 loss:  1.95481\n",
      "268 loss:  1.95447\n",
      "269 loss:  1.95378\n",
      "270 loss:  1.95346\n",
      "271 loss:  1.95315\n",
      "272 loss:  1.95237\n",
      "273 loss:  1.95271\n",
      "274 loss:  1.95234\n",
      "275 loss:  1.95162\n",
      "276 loss:  1.95136\n",
      "277 loss:  1.95094\n",
      "278 loss:  1.95049\n",
      "279 loss:  1.95034\n",
      "280 loss:  1.94971\n",
      "281 loss:  1.94937\n",
      "282 loss:  1.94906\n",
      "283 loss:  1.94887\n",
      "284 loss:  1.94837\n",
      "285 loss:  1.94852\n",
      "286 loss:  1.94851\n",
      "287 loss:  1.94789\n",
      "288 loss:  1.94839\n",
      "289 loss:  1.94866\n",
      "290 loss:  1.94793\n",
      "291 loss:  1.94747\n",
      "292 loss:  1.94647\n",
      "293 loss:  1.94689\n",
      "294 loss:  1.94567\n",
      "295 loss:  1.9466\n",
      "296 loss:  1.94643\n",
      "297 loss:  1.94657\n",
      "298 loss:  1.94495\n",
      "299 loss:  1.9466\n",
      "300 loss:  1.94845\n",
      "301 loss:  1.95009\n",
      "302 loss:  1.94696\n",
      "303 loss:  1.95\n",
      "304 loss:  1.95247\n",
      "305 loss:  1.95378\n",
      "306 loss:  1.95303\n",
      "307 loss:  1.95192\n",
      "308 loss:  1.94843\n",
      "309 loss:  1.94812\n",
      "310 loss:  1.9478\n",
      "311 loss:  1.94892\n",
      "312 loss:  1.94722\n",
      "313 loss:  1.94659\n",
      "314 loss:  1.94591\n",
      "315 loss:  1.94498\n",
      "316 loss:  1.94445\n",
      "317 loss:  1.94356\n",
      "318 loss:  1.94375\n",
      "319 loss:  1.94349\n",
      "320 loss:  1.94273\n",
      "321 loss:  1.94173\n",
      "322 loss:  1.94123\n",
      "323 loss:  1.9414\n",
      "324 loss:  1.9407\n",
      "325 loss:  1.94029\n",
      "326 loss:  1.93943\n",
      "327 loss:  1.93915\n",
      "328 loss:  1.93845\n",
      "329 loss:  1.93834\n",
      "330 loss:  1.93826\n",
      "331 loss:  1.93731\n",
      "332 loss:  1.9372\n",
      "333 loss:  1.93671\n",
      "334 loss:  1.93645\n",
      "335 loss:  1.93646\n",
      "336 loss:  1.93563\n",
      "337 loss:  1.93561\n",
      "338 loss:  1.93534\n",
      "339 loss:  1.93534\n",
      "340 loss:  1.935\n",
      "341 loss:  1.93439\n",
      "342 loss:  1.93454\n",
      "343 loss:  1.93464\n",
      "344 loss:  1.93396\n",
      "345 loss:  1.93392\n",
      "346 loss:  1.93358\n",
      "347 loss:  1.93328\n",
      "348 loss:  1.93295\n",
      "349 loss:  1.9333\n",
      "350 loss:  1.93531\n",
      "351 loss:  1.93547\n",
      "352 loss:  1.93263\n",
      "353 loss:  1.93863\n",
      "354 loss:  1.95107\n",
      "355 loss:  1.97089\n",
      "356 loss:  1.96146\n",
      "357 loss:  1.96426\n",
      "358 loss:  1.96751\n",
      "359 loss:  1.9635\n",
      "360 loss:  1.95821\n",
      "361 loss:  1.95645\n",
      "362 loss:  1.95555\n",
      "363 loss:  1.95067\n",
      "364 loss:  1.95216\n",
      "365 loss:  1.95467\n",
      "366 loss:  1.95738\n",
      "367 loss:  1.95169\n",
      "368 loss:  1.9492\n",
      "369 loss:  1.95208\n",
      "370 loss:  1.94792\n",
      "371 loss:  1.94839\n",
      "372 loss:  1.9497\n",
      "373 loss:  1.94941\n",
      "374 loss:  1.94844\n",
      "375 loss:  1.94942\n",
      "376 loss:  1.94695\n",
      "377 loss:  1.94756\n",
      "378 loss:  1.94686\n",
      "379 loss:  1.94544\n",
      "380 loss:  1.94377\n",
      "381 loss:  1.94263\n",
      "382 loss:  1.94369\n",
      "383 loss:  1.9415\n",
      "384 loss:  1.94242\n",
      "385 loss:  1.94307\n",
      "386 loss:  1.94019\n",
      "387 loss:  1.93981\n",
      "388 loss:  1.93911\n",
      "389 loss:  1.93946\n",
      "390 loss:  1.97577\n",
      "391 loss:  1.9569\n",
      "392 loss:  1.96642\n",
      "393 loss:  1.9713\n",
      "394 loss:  1.96762\n",
      "395 loss:  1.96751\n",
      "396 loss:  1.96299\n",
      "397 loss:  1.95366\n",
      "398 loss:  1.94871\n",
      "399 loss:  1.94803\n",
      "400 loss:  1.95006\n",
      "401 loss:  1.95117\n",
      "402 loss:  1.9517\n",
      "403 loss:  1.95056\n",
      "404 loss:  1.94793\n",
      "405 loss:  1.94609\n",
      "406 loss:  1.94482\n",
      "407 loss:  1.94462\n",
      "408 loss:  1.94465\n",
      "409 loss:  1.94334\n",
      "410 loss:  1.94192\n",
      "411 loss:  1.94022\n",
      "412 loss:  1.93986\n",
      "413 loss:  1.93872\n",
      "414 loss:  1.93793\n",
      "415 loss:  1.93749\n",
      "416 loss:  1.93666\n",
      "417 loss:  1.93581\n",
      "418 loss:  1.93478\n",
      "419 loss:  1.93383\n",
      "420 loss:  1.93318\n",
      "421 loss:  1.93303\n",
      "422 loss:  1.9324\n",
      "423 loss:  1.93161\n",
      "424 loss:  1.93212\n",
      "425 loss:  1.93235\n",
      "426 loss:  1.93267\n",
      "427 loss:  1.93195\n",
      "428 loss:  1.9317\n",
      "429 loss:  1.93094\n",
      "430 loss:  1.93005\n",
      "431 loss:  1.92955\n",
      "432 loss:  1.92922\n",
      "433 loss:  1.92981\n",
      "434 loss:  1.92885\n",
      "435 loss:  1.92889\n",
      "436 loss:  1.92835\n",
      "437 loss:  1.928\n",
      "438 loss:  1.92749\n",
      "439 loss:  1.92732\n",
      "440 loss:  1.92688\n",
      "441 loss:  1.92647\n",
      "442 loss:  1.92629\n",
      "443 loss:  1.92581\n",
      "444 loss:  1.92579\n",
      "445 loss:  1.92565\n",
      "446 loss:  1.9258\n",
      "447 loss:  1.92549\n",
      "448 loss:  1.9254\n",
      "449 loss:  1.92527\n",
      "450 loss:  1.92469\n",
      "451 loss:  1.92538\n",
      "452 loss:  1.92529\n",
      "453 loss:  1.92557\n",
      "454 loss:  1.92522\n",
      "455 loss:  1.92559\n",
      "456 loss:  1.92678\n",
      "457 loss:  1.92667\n",
      "458 loss:  1.92589\n",
      "459 loss:  1.92528\n",
      "460 loss:  1.92489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461 loss:  1.92514\n",
      "462 loss:  1.92465\n",
      "463 loss:  1.92434\n",
      "464 loss:  1.92408\n",
      "465 loss:  1.92385\n",
      "466 loss:  1.92359\n",
      "467 loss:  1.92311\n",
      "468 loss:  1.92296\n",
      "469 loss:  1.92414\n",
      "470 loss:  1.92431\n",
      "471 loss:  1.92337\n",
      "472 loss:  1.92336\n",
      "473 loss:  1.9229\n",
      "474 loss:  1.92257\n",
      "475 loss:  1.92229\n",
      "476 loss:  1.92193\n",
      "477 loss:  1.92196\n",
      "478 loss:  1.9216\n",
      "479 loss:  1.92131\n",
      "480 loss:  1.92124\n",
      "481 loss:  1.92085\n",
      "482 loss:  1.92081\n",
      "483 loss:  1.92082\n",
      "484 loss:  1.92196\n",
      "485 loss:  1.93429\n",
      "486 loss:  1.93004\n",
      "487 loss:  1.93017\n",
      "488 loss:  1.93411\n",
      "489 loss:  1.9354\n",
      "490 loss:  1.94135\n",
      "491 loss:  1.93383\n",
      "492 loss:  1.93469\n",
      "493 loss:  1.94193\n",
      "494 loss:  1.93814\n",
      "495 loss:  1.94908\n",
      "496 loss:  1.9395\n",
      "497 loss:  1.93884\n",
      "498 loss:  1.93957\n",
      "499 loss:  1.93704\n",
      "500 loss:  1.93177\n",
      "501 loss:  1.93192\n",
      "502 loss:  1.93361\n",
      "503 loss:  1.93506\n",
      "504 loss:  1.93529\n",
      "505 loss:  1.93185\n",
      "506 loss:  1.93028\n",
      "507 loss:  1.92957\n",
      "508 loss:  1.92888\n",
      "509 loss:  1.92766\n",
      "510 loss:  1.92673\n",
      "511 loss:  1.92669\n",
      "512 loss:  1.92682\n",
      "513 loss:  1.92628\n",
      "514 loss:  1.9255\n",
      "515 loss:  1.92479\n",
      "516 loss:  1.92451\n",
      "517 loss:  1.92403\n",
      "518 loss:  1.92366\n",
      "519 loss:  1.92317\n",
      "520 loss:  1.92255\n",
      "521 loss:  1.92261\n",
      "522 loss:  1.92215\n",
      "523 loss:  1.92197\n",
      "524 loss:  1.92181\n",
      "525 loss:  1.921\n",
      "526 loss:  1.9209\n",
      "527 loss:  1.92094\n",
      "528 loss:  1.92098\n",
      "529 loss:  1.92092\n",
      "530 loss:  1.92098\n",
      "531 loss:  1.92074\n",
      "532 loss:  1.92059\n",
      "533 loss:  1.92027\n",
      "534 loss:  1.91979\n",
      "535 loss:  1.91953\n",
      "536 loss:  1.9193\n",
      "537 loss:  1.91922\n",
      "538 loss:  1.91911\n",
      "539 loss:  1.91886\n",
      "540 loss:  1.91855\n",
      "541 loss:  1.91837\n",
      "542 loss:  1.91822\n",
      "543 loss:  1.91805\n",
      "544 loss:  1.91783\n",
      "545 loss:  1.91771\n",
      "546 loss:  1.9176\n",
      "547 loss:  1.91754\n",
      "548 loss:  1.91739\n",
      "549 loss:  1.91726\n",
      "550 loss:  1.9173\n",
      "551 loss:  1.91722\n",
      "552 loss:  1.91732\n",
      "553 loss:  1.91732\n",
      "554 loss:  1.91702\n",
      "555 loss:  1.91689\n",
      "556 loss:  1.91673\n",
      "557 loss:  1.91655\n",
      "558 loss:  1.91662\n",
      "559 loss:  1.91649\n",
      "560 loss:  1.91645\n",
      "561 loss:  1.91652\n",
      "562 loss:  1.91617\n",
      "563 loss:  1.91632\n",
      "564 loss:  1.91614\n",
      "565 loss:  1.91606\n",
      "566 loss:  1.91592\n",
      "567 loss:  1.91592\n",
      "568 loss:  1.91597\n",
      "569 loss:  1.91601\n",
      "570 loss:  1.9162\n",
      "571 loss:  1.91614\n",
      "572 loss:  1.91604\n",
      "573 loss:  1.91552\n",
      "574 loss:  1.91541\n",
      "575 loss:  1.91557\n",
      "576 loss:  1.9153\n",
      "577 loss:  1.91512\n",
      "578 loss:  1.91502\n",
      "579 loss:  1.91516\n",
      "580 loss:  1.91495\n",
      "581 loss:  1.91501\n",
      "582 loss:  1.91505\n",
      "583 loss:  1.91487\n",
      "584 loss:  1.9148\n",
      "585 loss:  1.91483\n",
      "586 loss:  1.91471\n",
      "587 loss:  1.91466\n",
      "588 loss:  1.91456\n",
      "589 loss:  1.91441\n",
      "590 loss:  1.91425\n",
      "591 loss:  1.91423\n",
      "592 loss:  1.91408\n",
      "593 loss:  1.91396\n",
      "594 loss:  1.91393\n",
      "595 loss:  1.91394\n",
      "596 loss:  1.91367\n",
      "597 loss:  1.9143\n",
      "598 loss:  1.92326\n",
      "599 loss:  1.92918\n",
      "600 loss:  1.93883\n",
      "601 loss:  1.94565\n",
      "602 loss:  1.94447\n",
      "603 loss:  1.94059\n",
      "604 loss:  1.9371\n",
      "605 loss:  1.93703\n",
      "606 loss:  1.93452\n",
      "607 loss:  1.93557\n",
      "608 loss:  1.93977\n",
      "609 loss:  1.93663\n",
      "610 loss:  1.9333\n",
      "611 loss:  1.93236\n",
      "612 loss:  1.93061\n",
      "613 loss:  1.92854\n",
      "614 loss:  1.92758\n",
      "615 loss:  1.9273\n",
      "616 loss:  1.9281\n",
      "617 loss:  1.92768\n",
      "618 loss:  1.92658\n",
      "619 loss:  1.92445\n",
      "620 loss:  1.92403\n",
      "621 loss:  1.92249\n",
      "622 loss:  1.92207\n",
      "623 loss:  1.92151\n",
      "624 loss:  1.92071\n",
      "625 loss:  1.9208\n",
      "626 loss:  1.92049\n",
      "627 loss:  1.92055\n",
      "628 loss:  1.91988\n",
      "629 loss:  1.91969\n",
      "630 loss:  1.9187\n",
      "631 loss:  1.91849\n",
      "632 loss:  1.91826\n",
      "633 loss:  1.91768\n",
      "634 loss:  1.91728\n",
      "635 loss:  1.91709\n",
      "636 loss:  1.91683\n",
      "637 loss:  1.91681\n",
      "638 loss:  1.91672\n",
      "639 loss:  1.91679\n",
      "640 loss:  1.91603\n",
      "641 loss:  1.91624\n",
      "642 loss:  1.91582\n",
      "643 loss:  1.91546\n",
      "644 loss:  1.91534\n",
      "645 loss:  1.91515\n",
      "646 loss:  1.91476\n",
      "647 loss:  1.91451\n",
      "648 loss:  1.91428\n",
      "649 loss:  1.9141\n",
      "650 loss:  1.91394\n",
      "651 loss:  1.91361\n",
      "652 loss:  1.9132\n",
      "653 loss:  1.91307\n",
      "654 loss:  1.91276\n",
      "655 loss:  1.91267\n",
      "656 loss:  1.91255\n",
      "657 loss:  1.91236\n",
      "658 loss:  1.91225\n",
      "659 loss:  1.91222\n",
      "660 loss:  1.91195\n",
      "661 loss:  1.91182\n",
      "662 loss:  1.91167\n",
      "663 loss:  1.91176\n",
      "664 loss:  1.91189\n",
      "665 loss:  1.91151\n",
      "666 loss:  1.9115\n",
      "667 loss:  1.91131\n",
      "668 loss:  1.9113\n",
      "669 loss:  1.91094\n",
      "670 loss:  1.91091\n",
      "671 loss:  1.9107\n",
      "672 loss:  1.91069\n",
      "673 loss:  1.91046\n",
      "674 loss:  1.91032\n",
      "675 loss:  1.91024\n",
      "676 loss:  1.91017\n",
      "677 loss:  1.91001\n",
      "678 loss:  1.90996\n",
      "679 loss:  1.90984\n",
      "680 loss:  1.90974\n",
      "681 loss:  1.90962\n",
      "682 loss:  1.90949\n",
      "683 loss:  1.90943\n",
      "684 loss:  1.90932\n",
      "685 loss:  1.90922\n",
      "686 loss:  1.90916\n",
      "687 loss:  1.90908\n",
      "688 loss:  1.90901\n",
      "689 loss:  1.90891\n",
      "690 loss:  1.90879\n",
      "691 loss:  1.90872\n",
      "692 loss:  1.90863\n",
      "693 loss:  1.90856\n",
      "694 loss:  1.90858\n",
      "695 loss:  1.90848\n",
      "696 loss:  1.90848\n",
      "697 loss:  1.90833\n",
      "698 loss:  1.90824\n",
      "699 loss:  1.90814\n",
      "700 loss:  1.90809\n",
      "701 loss:  1.90828\n",
      "702 loss:  1.90808\n",
      "703 loss:  1.90886\n",
      "704 loss:  1.90859\n",
      "705 loss:  1.90818\n",
      "706 loss:  1.91122\n",
      "707 loss:  1.94746\n",
      "708 loss:  1.92367\n",
      "709 loss:  1.99501\n",
      "710 loss:  2.0381\n",
      "711 loss:  2.06589\n",
      "712 loss:  2.06704\n",
      "713 loss:  2.07662\n",
      "714 loss:  2.05816\n",
      "715 loss:  2.10222\n",
      "716 loss:  2.18456\n",
      "717 loss:  2.16696\n",
      "718 loss:  2.10668\n",
      "719 loss:  2.07449\n",
      "720 loss:  2.06062\n",
      "721 loss:  2.07059\n",
      "722 loss:  2.07308\n",
      "723 loss:  2.07289\n",
      "724 loss:  2.06384\n",
      "725 loss:  2.04894\n",
      "726 loss:  2.04515\n",
      "727 loss:  2.03806\n",
      "728 loss:  2.0288\n",
      "729 loss:  2.0199\n",
      "730 loss:  2.01649\n",
      "731 loss:  2.01198\n",
      "732 loss:  2.00949\n",
      "733 loss:  2.00424\n",
      "734 loss:  1.99991\n",
      "735 loss:  1.99745\n",
      "736 loss:  1.99631\n",
      "737 loss:  1.99315\n",
      "738 loss:  1.99087\n",
      "739 loss:  1.98877\n",
      "740 loss:  1.98639\n",
      "741 loss:  1.98366\n",
      "742 loss:  1.98063\n",
      "743 loss:  1.97877\n",
      "744 loss:  1.9759\n",
      "745 loss:  1.97278\n",
      "746 loss:  1.97115\n",
      "747 loss:  1.97024\n",
      "748 loss:  1.96863\n",
      "749 loss:  1.96662\n",
      "750 loss:  1.96504\n",
      "751 loss:  1.9646\n",
      "752 loss:  1.96291\n",
      "753 loss:  1.96207\n",
      "754 loss:  1.96109\n",
      "755 loss:  1.96016\n",
      "756 loss:  1.95942\n",
      "757 loss:  1.95867\n",
      "758 loss:  1.95774\n",
      "759 loss:  1.95729\n",
      "760 loss:  1.95671\n",
      "761 loss:  1.95586\n",
      "762 loss:  1.95526\n",
      "763 loss:  1.95457\n",
      "764 loss:  1.954\n",
      "765 loss:  1.95336\n",
      "766 loss:  1.95265\n",
      "767 loss:  1.95224\n",
      "768 loss:  1.95161\n",
      "769 loss:  1.95112\n",
      "770 loss:  1.95058\n",
      "771 loss:  1.95011\n",
      "772 loss:  1.94942\n",
      "773 loss:  1.94891\n",
      "774 loss:  1.94839\n",
      "775 loss:  1.94802\n",
      "776 loss:  1.94754\n",
      "777 loss:  1.9473\n",
      "778 loss:  1.94723\n",
      "779 loss:  1.94705\n",
      "780 loss:  1.94645\n",
      "781 loss:  1.94627\n",
      "782 loss:  1.94612\n",
      "783 loss:  1.94577\n",
      "784 loss:  1.94542\n",
      "785 loss:  1.945\n",
      "786 loss:  1.94502\n",
      "787 loss:  1.94459\n",
      "788 loss:  1.9446\n",
      "789 loss:  1.94428\n",
      "790 loss:  1.94384\n",
      "791 loss:  1.9437\n",
      "792 loss:  1.94325\n",
      "793 loss:  1.94346\n",
      "794 loss:  1.94347\n",
      "795 loss:  1.94332\n",
      "796 loss:  1.9431\n",
      "797 loss:  1.94274\n",
      "798 loss:  1.94226\n",
      "799 loss:  1.94189\n",
      "800 loss:  1.94222\n",
      "801 loss:  1.94172\n",
      "802 loss:  1.94176\n",
      "803 loss:  1.9416\n",
      "804 loss:  1.94146\n",
      "805 loss:  1.94098\n",
      "806 loss:  1.94066\n",
      "807 loss:  1.94091\n",
      "808 loss:  1.9409\n",
      "809 loss:  1.94106\n",
      "810 loss:  1.94084\n",
      "811 loss:  1.94127\n",
      "812 loss:  1.94005\n",
      "813 loss:  1.94054\n",
      "814 loss:  1.94036\n",
      "815 loss:  1.93988\n",
      "816 loss:  1.9396\n",
      "817 loss:  1.93944\n",
      "818 loss:  1.93935\n",
      "819 loss:  1.93898\n",
      "820 loss:  1.93875\n",
      "821 loss:  1.93858\n",
      "822 loss:  1.93855\n",
      "823 loss:  1.93827\n",
      "824 loss:  1.93818\n",
      "825 loss:  1.93813\n",
      "826 loss:  1.9379\n",
      "827 loss:  1.93784\n",
      "828 loss:  1.93763\n",
      "829 loss:  1.93754\n",
      "830 loss:  1.93732\n",
      "831 loss:  1.93722\n",
      "832 loss:  1.93702\n",
      "833 loss:  1.93684\n",
      "834 loss:  1.93681\n",
      "835 loss:  1.93681\n",
      "836 loss:  1.93673\n",
      "837 loss:  1.93646\n",
      "838 loss:  1.9365\n",
      "839 loss:  1.93681\n",
      "840 loss:  1.9367\n",
      "841 loss:  1.93674\n",
      "842 loss:  1.93628\n",
      "843 loss:  1.93624\n",
      "844 loss:  1.93602\n",
      "845 loss:  1.93606\n",
      "846 loss:  1.93569\n",
      "847 loss:  1.93569\n",
      "848 loss:  1.93568\n",
      "849 loss:  1.93572\n",
      "850 loss:  1.93565\n",
      "851 loss:  1.93588\n",
      "852 loss:  1.93587\n",
      "853 loss:  1.93592\n",
      "854 loss:  1.9369\n",
      "855 loss:  1.94128\n",
      "856 loss:  1.94076\n",
      "857 loss:  1.94028\n",
      "858 loss:  1.93928\n",
      "859 loss:  1.93862\n",
      "860 loss:  1.93837\n",
      "861 loss:  1.9383\n",
      "862 loss:  1.9379\n",
      "863 loss:  1.93802\n",
      "864 loss:  1.93811\n",
      "865 loss:  1.93777\n",
      "866 loss:  1.93783\n",
      "867 loss:  1.93725\n",
      "868 loss:  1.93698\n",
      "869 loss:  1.93677\n",
      "870 loss:  1.93657\n",
      "871 loss:  1.936\n",
      "872 loss:  1.93568\n",
      "873 loss:  1.93547\n",
      "874 loss:  1.93535\n",
      "875 loss:  1.93533\n",
      "876 loss:  1.93504\n",
      "877 loss:  1.93482\n",
      "878 loss:  1.93455\n",
      "879 loss:  1.93449\n",
      "880 loss:  1.93437\n",
      "881 loss:  1.93413\n",
      "882 loss:  1.93395\n",
      "883 loss:  1.93376\n",
      "884 loss:  1.93354\n",
      "885 loss:  1.9334\n",
      "886 loss:  1.93338\n",
      "887 loss:  1.93316\n",
      "888 loss:  1.93332\n",
      "889 loss:  1.93483\n",
      "890 loss:  1.9354\n",
      "891 loss:  1.93483\n",
      "892 loss:  1.93498\n",
      "893 loss:  1.93643\n",
      "894 loss:  1.93724\n",
      "895 loss:  1.93604\n",
      "896 loss:  1.93684\n",
      "897 loss:  1.94289\n",
      "898 loss:  1.94402\n",
      "899 loss:  1.94515\n",
      "900 loss:  1.94664\n",
      "901 loss:  1.94529\n",
      "902 loss:  1.9444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903 loss:  1.94084\n",
      "904 loss:  1.9417\n",
      "905 loss:  1.94249\n",
      "906 loss:  1.94127\n",
      "907 loss:  1.94011\n",
      "908 loss:  1.94047\n",
      "909 loss:  1.94144\n",
      "910 loss:  1.93981\n",
      "911 loss:  1.93844\n",
      "912 loss:  1.9377\n",
      "913 loss:  1.93756\n",
      "914 loss:  1.93685\n",
      "915 loss:  1.93633\n",
      "916 loss:  1.93593\n",
      "917 loss:  1.93533\n",
      "918 loss:  1.93526\n",
      "919 loss:  1.93481\n",
      "920 loss:  1.9343\n",
      "921 loss:  1.9341\n",
      "922 loss:  1.93351\n",
      "923 loss:  1.93323\n",
      "924 loss:  1.933\n",
      "925 loss:  1.93274\n",
      "926 loss:  1.93219\n",
      "927 loss:  1.93198\n",
      "928 loss:  1.93183\n",
      "929 loss:  1.93168\n",
      "930 loss:  1.93151\n",
      "931 loss:  1.93125\n",
      "932 loss:  1.93105\n",
      "933 loss:  1.93083\n",
      "934 loss:  1.93067\n",
      "935 loss:  1.93051\n",
      "936 loss:  1.93034\n",
      "937 loss:  1.93019\n",
      "938 loss:  1.93011\n",
      "939 loss:  1.92994\n",
      "940 loss:  1.92981\n",
      "941 loss:  1.92963\n",
      "942 loss:  1.92953\n",
      "943 loss:  1.92944\n",
      "944 loss:  1.92929\n",
      "945 loss:  1.92913\n",
      "946 loss:  1.92898\n",
      "947 loss:  1.92882\n",
      "948 loss:  1.92864\n",
      "949 loss:  1.92838\n",
      "950 loss:  1.92809\n",
      "951 loss:  1.92801\n",
      "952 loss:  1.92828\n",
      "953 loss:  1.92803\n",
      "954 loss:  1.92805\n",
      "955 loss:  1.92826\n",
      "956 loss:  1.92797\n",
      "957 loss:  1.92817\n",
      "958 loss:  1.92829\n",
      "959 loss:  1.92804\n",
      "960 loss:  1.92807\n",
      "961 loss:  1.92784\n",
      "962 loss:  1.92763\n",
      "963 loss:  1.92756\n",
      "964 loss:  1.92745\n",
      "965 loss:  1.92739\n",
      "966 loss:  1.9273\n",
      "967 loss:  1.92708\n",
      "968 loss:  1.92697\n",
      "969 loss:  1.92686\n",
      "970 loss:  1.9268\n",
      "971 loss:  1.92684\n",
      "972 loss:  1.92684\n",
      "973 loss:  1.92656\n",
      "974 loss:  1.92657\n",
      "975 loss:  1.92649\n",
      "976 loss:  1.92639\n",
      "977 loss:  1.92617\n",
      "978 loss:  1.92625\n",
      "979 loss:  1.92609\n",
      "980 loss:  1.92609\n",
      "981 loss:  1.92593\n",
      "982 loss:  1.92578\n",
      "983 loss:  1.92574\n",
      "984 loss:  1.92567\n",
      "985 loss:  1.92563\n",
      "986 loss:  1.92553\n",
      "987 loss:  1.92545\n",
      "988 loss:  1.92534\n",
      "989 loss:  1.92533\n",
      "990 loss:  1.92519\n",
      "991 loss:  1.92513\n",
      "992 loss:  1.92515\n",
      "993 loss:  1.92505\n",
      "994 loss:  1.92495\n",
      "995 loss:  1.92488\n",
      "996 loss:  1.92482\n",
      "997 loss:  1.92475\n",
      "998 loss:  1.92458\n",
      "999 loss:  1.92441\n",
      "\tPrediction str:  t you  ant\n",
      "\tAnswer str:  f you want\n",
      "\tAnswer str2:  ['f', ' ', 'y', 'o', 'u', ' ', 'w', 'a', 'n', 't']\n",
      "\tPrediction str:  otou  ant \n",
      "\tAnswer str:   you want \n",
      "\tAnswer str2:  [' ', 'y', 'o', 'u', ' ', 'w', 'a', 'n', 't', ' ']\n",
      "\tPrediction str:  tou  ant t\n",
      "\tAnswer str:  you want t\n",
      "\tAnswer str2:  ['y', 'o', 'u', ' ', 'w', 'a', 'n', 't', ' ', 't']\n",
      "\tPrediction str:   u  ant to\n",
      "\tAnswer str:  ou want to\n",
      "\tAnswer str2:  ['o', 'u', ' ', 'w', 'a', 'n', 't', ' ', 't', 'o']\n",
      "\tPrediction str:  n tant to \n",
      "\tAnswer str:  u want to \n",
      "\tAnswer str2:  ['u', ' ', 'w', 'a', 'n', 't', ' ', 't', 'o', ' ']\n",
      "\tPrediction str:   iant to t\n",
      "\tAnswer str:   want to b\n",
      "\tAnswer str2:  [' ', 'w', 'a', 'n', 't', ' ', 't', 'o', ' ', 'b']\n",
      "\tPrediction str:  tont to t \n",
      "\tAnswer str:  want to bu\n",
      "\tAnswer str2:  ['w', 'a', 'n', 't', ' ', 't', 'o', ' ', 'b', 'u']\n",
      "\tPrediction str:  ont to t  \n",
      "\tAnswer str:  ant to bui\n",
      "\tAnswer str2:  ['a', 'n', 't', ' ', 't', 'o', ' ', 'b', 'u', 'i']\n",
      "\tPrediction str:  nt th lt l\n",
      "\tAnswer str:  nt to buil\n",
      "\tAnswer str2:  ['n', 't', ' ', 't', 'o', ' ', 'b', 'u', 'i', 'l']\n",
      "\tPrediction str:  d to l  ld\n",
      "\tAnswer str:  t to build\n",
      "\tAnswer str2:  ['t', ' ', 't', 'o', ' ', 'b', 'u', 'i', 'l', 'd']\n",
      "\tPrediction str:   ao l  ld \n",
      "\tAnswer str:   to build \n",
      "\tAnswer str2:  [' ', 't', 'o', ' ', 'b', 'u', 'i', 'l', 'd', ' ']\n",
      "\tPrediction str:  to l  ld a\n",
      "\tAnswer str:  to build a\n",
      "\tAnswer str2:  ['t', 'o', ' ', 'b', 'u', 'i', 'l', 'd', ' ', 'a']\n",
      "\tPrediction str:    l  ld a \n",
      "\tAnswer str:  o build a \n",
      "\tAnswer str2:  ['o', ' ', 'b', 'u', 'i', 'l', 'd', ' ', 'a', ' ']\n",
      "\tPrediction str:  nll ld a s\n",
      "\tAnswer str:   build a s\n",
      "\tAnswer str2:  [' ', 'b', 'u', 'i', 'l', 'd', ' ', 'a', ' ', 's']\n",
      "\tPrediction str:  tutld a sh\n",
      "\tAnswer str:  build a sh\n",
      "\tAnswer str2:  ['b', 'u', 'i', 'l', 'd', ' ', 'a', ' ', 's', 'h']\n",
      "\tPrediction str:  utld a shi\n",
      "\tAnswer str:  uild a shi\n",
      "\tAnswer str2:  ['u', 'i', 'l', 'd', ' ', 'a', ' ', 's', 'h', 'i']\n",
      "\tPrediction str:   ld a shi,\n",
      "\tAnswer str:  ild a ship\n",
      "\tAnswer str2:  ['i', 'l', 'd', ' ', 'a', ' ', 's', 'h', 'i', 'p']\n",
      "\tPrediction str:  tl a shi,,\n",
      "\tAnswer str:  ld a ship,\n",
      "\tAnswer str2:  ['l', 'd', ' ', 'a', ' ', 's', 'h', 'i', 'p', ',']\n",
      "\tPrediction str:  e a shi,, \n",
      "\tAnswer str:  d a ship, \n",
      "\tAnswer str2:  ['d', ' ', 'a', ' ', 's', 'h', 'i', 'p', ',', ' ']\n",
      "\tPrediction str:   anshi,, d\n",
      "\tAnswer str:   a ship, d\n",
      "\tAnswer str2:  [' ', 'a', ' ', 's', 'h', 'i', 'p', ',', ' ', 'd']\n",
      "\tPrediction str:  tsthi,, do\n",
      "\tAnswer str:  a ship, do\n",
      "\tAnswer str2:  ['a', ' ', 's', 'h', 'i', 'p', ',', ' ', 'd', 'o']\n",
      "\tPrediction str:  nthi,, don\n",
      "\tAnswer str:   ship, don\n",
      "\tAnswer str2:  [' ', 's', 'h', 'i', 'p', ',', ' ', 'd', 'o', 'n']\n",
      "\tPrediction str:  thi,, dond\n",
      "\tAnswer str:  ship, don'\n",
      "\tAnswer str2:  ['s', 'h', 'i', 'p', ',', ' ', 'd', 'o', 'n', \"'\"]\n",
      "\tPrediction str:  ii,  dondt\n",
      "\tAnswer str:  hip, don't\n",
      "\tAnswer str2:  ['h', 'i', 'p', ',', ' ', 'd', 'o', 'n', \"'\", 't']\n",
      "\tPrediction str:  et  dondt \n",
      "\tAnswer str:  ip, don't \n",
      "\tAnswer str2:  ['i', 'p', ',', ' ', 'd', 'o', 'n', \"'\", 't', ' ']\n",
      "\tPrediction str:  tl dondt a\n",
      "\tAnswer str:  p, don't d\n",
      "\tAnswer str2:  ['p', ',', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'd']\n",
      "\tPrediction str:    dondt ar\n",
      "\tAnswer str:  , don't dr\n",
      "\tAnswer str2:  [',', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'd', 'r']\n",
      "\tPrediction str:   dondt aru\n",
      "\tAnswer str:   don't dru\n",
      "\tAnswer str2:  [' ', 'd', 'o', 'n', \"'\", 't', ' ', 'd', 'r', 'u']\n",
      "\tPrediction str:  tondt aru \n",
      "\tAnswer str:  don't drum\n",
      "\tAnswer str2:  ['d', 'o', 'n', \"'\", 't', ' ', 'd', 'r', 'u', 'm']\n",
      "\tPrediction str:   ndt aru  \n",
      "\tAnswer str:  on't drum \n",
      "\tAnswer str2:  ['o', 'n', \"'\", 't', ' ', 'd', 'r', 'u', 'm', ' ']\n",
      "\tPrediction str:  ndt aru  u\n",
      "\tAnswer str:  n't drum u\n",
      "\tAnswer str2:  ['n', \"'\", 't', ' ', 'd', 'r', 'u', 'm', ' ', 'u']\n",
      "\tPrediction str:  dt aru  u \n",
      "\tAnswer str:  't drum up\n",
      "\tAnswer str2:  [\"'\", 't', ' ', 'd', 'r', 'u', 'm', ' ', 'u', 'p']\n",
      "\tPrediction str:  t aru  u  \n",
      "\tAnswer str:  t drum up \n",
      "\tAnswer str2:  ['t', ' ', 'd', 'r', 'u', 'm', ' ', 'u', 'p', ' ']\n",
      "\tPrediction str:   aoauuu   \n",
      "\tAnswer str:   drum up p\n",
      "\tAnswer str2:  [' ', 'd', 'r', 'u', 'm', ' ', 'u', 'p', ' ', 'p']\n",
      "\tPrediction str:  toa  u  ee\n",
      "\tAnswer str:  drum up pe\n",
      "\tAnswer str2:  ['d', 'r', 'u', 'm', ' ', 'u', 'p', ' ', 'p', 'e']\n",
      "\tPrediction str:   a  u  eeo\n",
      "\tAnswer str:  rum up peo\n",
      "\tAnswer str2:  ['r', 'u', 'm', ' ', 'u', 'p', ' ', 'p', 'e', 'o']\n",
      "\tPrediction str:     u  eeoe\n",
      "\tAnswer str:  um up peop\n",
      "\tAnswer str2:  ['u', 'm', ' ', 'u', 'p', ' ', 'p', 'e', 'o', 'p']\n",
      "\tPrediction str:    u  eeole\n",
      "\tAnswer str:  m up peopl\n",
      "\tAnswer str2:  ['m', ' ', 'u', 'p', ' ', 'p', 'e', 'o', 'p', 'l']\n",
      "\tPrediction str:  et  eeolee\n",
      "\tAnswer str:   up people\n",
      "\tAnswer str2:  [' ', 'u', 'p', ' ', 'p', 'e', 'o', 'p', 'l', 'e']\n",
      "\tPrediction str:  t  eeolee \n",
      "\tAnswer str:  up people \n",
      "\tAnswer str2:  ['u', 'p', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ']\n",
      "\tPrediction str:    eeolee t\n",
      "\tAnswer str:  p people t\n",
      "\tAnswer str2:  ['p', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 't']\n",
      "\tPrediction str:   eeolle to\n",
      "\tAnswer str:   people to\n",
      "\tAnswer str2:  [' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 't', 'o']\n",
      "\tPrediction str:  teolee tog\n",
      "\tAnswer str:  people tog\n",
      "\tAnswer str2:  ['p', 'e', 'o', 'p', 'l', 'e', ' ', 't', 'o', 'g']\n",
      "\tPrediction str:    lee toge\n",
      "\tAnswer str:  eople toge\n",
      "\tAnswer str2:  ['e', 'o', 'p', 'l', 'e', ' ', 't', 'o', 'g', 'e']\n",
      "\tPrediction str:  mele toget\n",
      "\tAnswer str:  ople toget\n",
      "\tAnswer str2:  ['o', 'p', 'l', 'e', ' ', 't', 'o', 'g', 'e', 't']\n",
      "\tPrediction str:  nle togeth\n",
      "\tAnswer str:  ple togeth\n",
      "\tAnswer str2:  ['p', 'l', 'e', ' ', 't', 'o', 'g', 'e', 't', 'h']\n",
      "\tPrediction str:   e togethe\n",
      "\tAnswer str:  le togethe\n",
      "\tAnswer str2:  ['l', 'e', ' ', 't', 'o', 'g', 'e', 't', 'h', 'e']\n",
      "\tPrediction str:  esso ether\n",
      "\tAnswer str:  e together\n",
      "\tAnswer str2:  ['e', ' ', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r']\n",
      "\tPrediction str:  mto ether \n",
      "\tAnswer str:   together \n",
      "\tAnswer str2:  [' ', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r', ' ']\n",
      "\tPrediction str:  to ether t\n",
      "\tAnswer str:  together t\n",
      "\tAnswer str2:  ['t', 'o', 'g', 'e', 't', 'h', 'e', 'r', ' ', 't']\n",
      "\tPrediction str:    ether to\n",
      "\tAnswer str:  ogether to\n",
      "\tAnswer str2:  ['o', 'g', 'e', 't', 'h', 'e', 'r', ' ', 't', 'o']\n",
      "\tPrediction str:  nether to \n",
      "\tAnswer str:  gether to \n",
      "\tAnswer str2:  ['g', 'e', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ']\n",
      "\tPrediction str:  ether to e\n",
      "\tAnswer str:  ether to c\n",
      "\tAnswer str2:  ['e', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'c']\n",
      "\tPrediction str:  mser to eo\n",
      "\tAnswer str:  ther to co\n",
      "\tAnswer str2:  ['t', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'c', 'o']\n",
      "\tPrediction str:   er to eol\n",
      "\tAnswer str:  her to col\n",
      "\tAnswer str2:  ['h', 'e', 'r', ' ', 't', 'o', ' ', 'c', 'o', 'l']\n",
      "\tPrediction str:  em to loll\n",
      "\tAnswer str:  er to coll\n",
      "\tAnswer str2:  ['e', 'r', ' ', 't', 'o', ' ', 'c', 'o', 'l', 'l']\n",
      "\tPrediction str:  m to lolle\n",
      "\tAnswer str:  r to colle\n",
      "\tAnswer str2:  ['r', ' ', 't', 'o', ' ', 'c', 'o', 'l', 'l', 'e']\n",
      "\tPrediction str:   th lolle \n",
      "\tAnswer str:   to collec\n",
      "\tAnswer str2:  [' ', 't', 'o', ' ', 'c', 'o', 'l', 'l', 'e', 'c']\n",
      "\tPrediction str:  to lolle t\n",
      "\tAnswer str:  to collect\n",
      "\tAnswer str2:  ['t', 'o', ' ', 'c', 'o', 'l', 'l', 'e', 'c', 't']\n",
      "\tPrediction str:    lolle t \n",
      "\tAnswer str:  o collect \n",
      "\tAnswer str2:  ['o', ' ', 'c', 'o', 'l', 'l', 'e', 'c', 't', ' ']\n",
      "\tPrediction str:  nlolle t  \n",
      "\tAnswer str:   collect w\n",
      "\tAnswer str2:  [' ', 'c', 'o', 'l', 'l', 'e', 'c', 't', ' ', 'w']\n",
      "\tPrediction str:  tolle t  o\n",
      "\tAnswer str:  collect wo\n",
      "\tAnswer str2:  ['c', 'o', 'l', 'l', 'e', 'c', 't', ' ', 'w', 'o']\n",
      "\tPrediction str:  h le t  oo\n",
      "\tAnswer str:  ollect woo\n",
      "\tAnswer str2:  ['o', 'l', 'l', 'e', 'c', 't', ' ', 'w', 'o', 'o']\n",
      "\tPrediction str:  nee t  ood\n",
      "\tAnswer str:  llect wood\n",
      "\tAnswer str2:  ['l', 'l', 'e', 'c', 't', ' ', 'w', 'o', 'o', 'd']\n",
      "\tPrediction str:  ee t  ood \n",
      "\tAnswer str:  lect wood \n",
      "\tAnswer str2:  ['l', 'e', 'c', 't', ' ', 'w', 'o', 'o', 'd', ' ']\n",
      "\tPrediction str:  est  ood a\n",
      "\tAnswer str:  ect wood a\n",
      "\tAnswer str2:  ['e', 'c', 't', ' ', 'w', 'o', 'o', 'd', ' ', 'a']\n",
      "\tPrediction str:  mt aood an\n",
      "\tAnswer str:  ct wood an\n",
      "\tAnswer str2:  ['c', 't', ' ', 'w', 'o', 'o', 'd', ' ', 'a', 'n']\n",
      "\tPrediction str:  h aood and\n",
      "\tAnswer str:  t wood and\n",
      "\tAnswer str2:  ['t', ' ', 'w', 'o', 'o', 'd', ' ', 'a', 'n', 'd']\n",
      "\tPrediction str:   aood and \n",
      "\tAnswer str:   wood and \n",
      "\tAnswer str2:  [' ', 'w', 'o', 'o', 'd', ' ', 'a', 'n', 'd', ' ']\n",
      "\tPrediction str:  tood and d\n",
      "\tAnswer str:  wood and d\n",
      "\tAnswer str2:  ['w', 'o', 'o', 'd', ' ', 'a', 'n', 'd', ' ', 'd']\n",
      "\tPrediction str:  ord and do\n",
      "\tAnswer str:  ood and do\n",
      "\tAnswer str2:  ['o', 'o', 'd', ' ', 'a', 'n', 'd', ' ', 'd', 'o']\n",
      "\tPrediction str:  nl and don\n",
      "\tAnswer str:  od and don\n",
      "\tAnswer str2:  ['o', 'd', ' ', 'a', 'n', 'd', ' ', 'd', 'o', 'n']\n",
      "\tPrediction str:  n a d dond\n",
      "\tAnswer str:  d and don'\n",
      "\tAnswer str2:  ['d', ' ', 'a', 'n', 'd', ' ', 'd', 'o', 'n', \"'\"]\n",
      "\tPrediction str:   and dondt\n",
      "\tAnswer str:   and don't\n",
      "\tAnswer str2:  [' ', 'a', 'n', 'd', ' ', 'd', 'o', 'n', \"'\", 't']\n",
      "\tPrediction str:  tst dondt \n",
      "\tAnswer str:  and don't \n",
      "\tAnswer str2:  ['a', 'n', 'd', ' ', 'd', 'o', 'n', \"'\", 't', ' ']\n",
      "\tPrediction str:  nt dondt a\n",
      "\tAnswer str:  nd don't a\n",
      "\tAnswer str2:  ['n', 'd', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'a']\n",
      "\tPrediction str:  d dondt as\n",
      "\tAnswer str:  d don't as\n",
      "\tAnswer str2:  ['d', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'a', 's']\n",
      "\tPrediction str:   aondt ass\n",
      "\tAnswer str:   don't ass\n",
      "\tAnswer str2:  [' ', 'd', 'o', 'n', \"'\", 't', ' ', 'a', 's', 's']\n",
      "\tPrediction str:  tondt assi\n",
      "\tAnswer str:  don't assi\n",
      "\tAnswer str2:  ['d', 'o', 'n', \"'\", 't', ' ', 'a', 's', 's', 'i']\n",
      "\tPrediction str:   ndt assig\n",
      "\tAnswer str:  on't assig\n",
      "\tAnswer str2:  ['o', 'n', \"'\", 't', ' ', 'a', 's', 's', 'i', 'g']\n",
      "\tPrediction str:  ndt assign\n",
      "\tAnswer str:  n't assign\n",
      "\tAnswer str2:  ['n', \"'\", 't', ' ', 'a', 's', 's', 'i', 'g', 'n']\n",
      "\tPrediction str:  dt assign \n",
      "\tAnswer str:  't assign \n",
      "\tAnswer str2:  [\"'\", 't', ' ', 'a', 's', 's', 'i', 'g', 'n', ' ']\n",
      "\tPrediction str:  t assign t\n",
      "\tAnswer str:  t assign t\n",
      "\tAnswer str2:  ['t', ' ', 'a', 's', 's', 'i', 'g', 'n', ' ', 't']\n",
      "\tPrediction str:   assign th\n",
      "\tAnswer str:   assign th\n",
      "\tAnswer str2:  [' ', 'a', 's', 's', 'i', 'g', 'n', ' ', 't', 'h']\n",
      "\tPrediction str:  tssign the\n",
      "\tAnswer str:  assign the\n",
      "\tAnswer str2:  ['a', 's', 's', 'i', 'g', 'n', ' ', 't', 'h', 'e']\n",
      "\tPrediction str:  nsign them\n",
      "\tAnswer str:  ssign them\n",
      "\tAnswer str2:  ['s', 's', 'i', 'g', 'n', ' ', 't', 'h', 'e', 'm']\n",
      "\tPrediction str:  iign them \n",
      "\tAnswer str:  sign them \n",
      "\tAnswer str2:  ['s', 'i', 'g', 'n', ' ', 't', 'h', 'e', 'm', ' ']\n",
      "\tPrediction str:  igntthem t\n",
      "\tAnswer str:  ign them t\n",
      "\tAnswer str2:  ['i', 'g', 'n', ' ', 't', 'h', 'e', 'm', ' ', 't']\n",
      "\tPrediction str:  tn them to\n",
      "\tAnswer str:  gn them ta\n",
      "\tAnswer str2:  ['g', 'n', ' ', 't', 'h', 'e', 'm', ' ', 't', 'a']\n",
      "\tPrediction str:  e them tos\n",
      "\tAnswer str:  n them tas\n",
      "\tAnswer str2:  ['n', ' ', 't', 'h', 'e', 'm', ' ', 't', 'a', 's']\n",
      "\tPrediction str:  dtoem toss\n",
      "\tAnswer str:   them task\n",
      "\tAnswer str2:  [' ', 't', 'h', 'e', 'm', ' ', 't', 'a', 's', 'k']\n",
      "\tPrediction str:  toer tosss\n",
      "\tAnswer str:  them tasks\n",
      "\tAnswer str2:  ['t', 'h', 'e', 'm', ' ', 't', 'a', 's', 'k', 's']\n",
      "\tPrediction str:   er tosss \n",
      "\tAnswer str:  hem tasks \n",
      "\tAnswer str2:  ['h', 'e', 'm', ' ', 't', 'a', 's', 'k', 's', ' ']\n",
      "\tPrediction str:  em tosss a\n",
      "\tAnswer str:  em tasks a\n",
      "\tAnswer str2:  ['e', 'm', ' ', 't', 'a', 's', 'k', 's', ' ', 'a']\n",
      "\tPrediction str:  m tosss an\n",
      "\tAnswer str:  m tasks an\n",
      "\tAnswer str2:  ['m', ' ', 't', 'a', 's', 'k', 's', ' ', 'a', 'n']\n",
      "\tPrediction str:  etosss and\n",
      "\tAnswer str:   tasks and\n",
      "\tAnswer str2:  [' ', 't', 'a', 's', 'k', 's', ' ', 'a', 'n', 'd']\n",
      "\tPrediction str:  tosss and \n",
      "\tAnswer str:  tasks and \n",
      "\tAnswer str2:  ['t', 'a', 's', 'k', 's', ' ', 'a', 'n', 'd', ' ']\n",
      "\tPrediction str:   sss and d\n",
      "\tAnswer str:  asks and w\n",
      "\tAnswer str2:  ['a', 's', 'k', 's', ' ', 'a', 'n', 'd', ' ', 'w']\n",
      "\tPrediction str:  nss and do\n",
      "\tAnswer str:  sks and wo\n",
      "\tAnswer str2:  ['s', 'k', 's', ' ', 'a', 'n', 'd', ' ', 'w', 'o']\n",
      "\tPrediction str:  is and dor\n",
      "\tAnswer str:  ks and wor\n",
      "\tAnswer str2:  ['k', 's', ' ', 'a', 'n', 'd', ' ', 'w', 'o', 'r']\n",
      "\tPrediction str:  s and dor \n",
      "\tAnswer str:  s and work\n",
      "\tAnswer str2:  ['s', ' ', 'a', 'n', 'd', ' ', 'w', 'o', 'r', 'k']\n",
      "\tPrediction str:  iind dor ,\n",
      "\tAnswer str:   and work,\n",
      "\tAnswer str2:  [' ', 'a', 'n', 'd', ' ', 'w', 'o', 'r', 'k', ',']\n",
      "\tPrediction str:  tst dor , \n",
      "\tAnswer str:  and work, \n",
      "\tAnswer str2:  ['a', 'n', 'd', ' ', 'w', 'o', 'r', 'k', ',', ' ']\n",
      "\tPrediction str:  nt dor ,  \n",
      "\tAnswer str:  nd work, b\n",
      "\tAnswer str2:  ['n', 'd', ' ', 'w', 'o', 'r', 'k', ',', ' ', 'b']\n",
      "\tPrediction str:  d dor , uu\n",
      "\tAnswer str:  d work, bu\n",
      "\tAnswer str2:  ['d', ' ', 'w', 'o', 'r', 'k', ',', ' ', 'b', 'u']\n",
      "\tPrediction str:   aor , uut\n",
      "\tAnswer str:   work, but\n",
      "\tAnswer str2:  [' ', 'w', 'o', 'r', 'k', ',', ' ', 'b', 'u', 't']\n",
      "\tPrediction str:  too , uut \n",
      "\tAnswer str:  work, but \n",
      "\tAnswer str2:  ['w', 'o', 'r', 'k', ',', ' ', 'b', 'u', 't', ' ']\n",
      "\tPrediction str:  or , uut r\n",
      "\tAnswer str:  ork, but r\n",
      "\tAnswer str2:  ['o', 'r', 'k', ',', ' ', 'b', 'u', 't', ' ', 'r']\n",
      "\tPrediction str:  n , uut ra\n",
      "\tAnswer str:  rk, but ra\n",
      "\tAnswer str2:  ['r', 'k', ',', ' ', 'b', 'u', 't', ' ', 'r', 'a']\n",
      "\tPrediction str:   u uut rat\n",
      "\tAnswer str:  k, but rat\n",
      "\tAnswer str2:  ['k', ',', ' ', 'b', 'u', 't', ' ', 'r', 'a', 't']\n",
      "\tPrediction str:  s uut rath\n",
      "\tAnswer str:  , but rath\n",
      "\tAnswer str2:  [',', ' ', 'b', 'u', 't', ' ', 'r', 'a', 't', 'h']\n",
      "\tPrediction str:   dut rathe\n",
      "\tAnswer str:   but rathe\n",
      "\tAnswer str2:  [' ', 'b', 'u', 't', ' ', 'r', 'a', 't', 'h', 'e']\n",
      "\tPrediction str:  tut rather\n",
      "\tAnswer str:  but rather\n",
      "\tAnswer str2:  ['b', 'u', 't', ' ', 'r', 'a', 't', 'h', 'e', 'r']\n",
      "\tPrediction str:  ut rather \n",
      "\tAnswer str:  ut rather \n",
      "\tAnswer str2:  ['u', 't', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ']\n",
      "\tPrediction str:    rather t\n",
      "\tAnswer str:  t rather t\n",
      "\tAnswer str2:  ['t', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't']\n",
      "\tPrediction str:   aather te\n",
      "\tAnswer str:   rather te\n",
      "\tAnswer str2:  [' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'e']\n",
      "\tPrediction str:  tather tea\n",
      "\tAnswer str:  rather tea\n",
      "\tAnswer str2:  ['r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'e', 'a']\n",
      "\tPrediction str:   ther tea \n",
      "\tAnswer str:  ather teac\n",
      "\tAnswer str2:  ['a', 't', 'h', 'e', 'r', ' ', 't', 'e', 'a', 'c']\n",
      "\tPrediction str:  nser tea h\n",
      "\tAnswer str:  ther teach\n",
      "\tAnswer str2:  ['t', 'h', 'e', 'r', ' ', 't', 'e', 'a', 'c', 'h']\n",
      "\tPrediction str:   er toa h \n",
      "\tAnswer str:  her teach \n",
      "\tAnswer str2:  ['h', 'e', 'r', ' ', 't', 'e', 'a', 'c', 'h', ' ']\n",
      "\tPrediction str:  em toa h t\n",
      "\tAnswer str:  er teach t\n",
      "\tAnswer str2:  ['e', 'r', ' ', 't', 'e', 'a', 'c', 'h', ' ', 't']\n",
      "\tPrediction str:  m toa h th\n",
      "\tAnswer str:  r teach th\n",
      "\tAnswer str2:  ['r', ' ', 't', 'e', 'a', 'c', 'h', ' ', 't', 'h']\n",
      "\tPrediction str:   tha h the\n",
      "\tAnswer str:   teach the\n",
      "\tAnswer str2:  [' ', 't', 'e', 'a', 'c', 'h', ' ', 't', 'h', 'e']\n",
      "\tPrediction str:  toash them\n",
      "\tAnswer str:  teach them\n",
      "\tAnswer str2:  ['t', 'e', 'a', 'c', 'h', ' ', 't', 'h', 'e', 'm']\n",
      "\tPrediction str:   ast them \n",
      "\tAnswer str:  each them \n",
      "\tAnswer str2:  ['e', 'a', 'c', 'h', ' ', 't', 'h', 'e', 'm', ' ']\n",
      "\tPrediction str:  mst them t\n",
      "\tAnswer str:  ach them t\n",
      "\tAnswer str2:  ['a', 'c', 'h', ' ', 't', 'h', 'e', 'm', ' ', 't']\n",
      "\tPrediction str:  nt them to\n",
      "\tAnswer str:  ch them to\n",
      "\tAnswer str2:  ['c', 'h', ' ', 't', 'h', 'e', 'm', ' ', 't', 'o']\n",
      "\tPrediction str:  h them to \n",
      "\tAnswer str:  h them to \n",
      "\tAnswer str2:  ['h', ' ', 't', 'h', 'e', 'm', ' ', 't', 'o', ' ']\n",
      "\tPrediction str:  ether to l\n",
      "\tAnswer str:   them to l\n",
      "\tAnswer str2:  [' ', 't', 'h', 'e', 'm', ' ', 't', 'o', ' ', 'l']\n",
      "\tPrediction str:  toer to lo\n",
      "\tAnswer str:  them to lo\n",
      "\tAnswer str2:  ['t', 'h', 'e', 'm', ' ', 't', 'o', ' ', 'l', 'o']\n",
      "\tPrediction str:   er to lon\n",
      "\tAnswer str:  hem to lon\n",
      "\tAnswer str2:  ['h', 'e', 'm', ' ', 't', 'o', ' ', 'l', 'o', 'n']\n",
      "\tPrediction str:  em to lond\n",
      "\tAnswer str:  em to long\n",
      "\tAnswer str2:  ['e', 'm', ' ', 't', 'o', ' ', 'l', 'o', 'n', 'g']\n",
      "\tPrediction str:  m to lend \n",
      "\tAnswer str:  m to long \n",
      "\tAnswer str2:  ['m', ' ', 't', 'o', ' ', 'l', 'o', 'n', 'g', ' ']\n",
      "\tPrediction str:  eto lend t\n",
      "\tAnswer str:   to long f\n",
      "\tAnswer str2:  [' ', 't', 'o', ' ', 'l', 'o', 'n', 'g', ' ', 'f']\n",
      "\tPrediction str:  to lond to\n",
      "\tAnswer str:  to long fo\n",
      "\tAnswer str2:  ['t', 'o', ' ', 'l', 'o', 'n', 'g', ' ', 'f', 'o']\n",
      "\tPrediction str:    lend tor\n",
      "\tAnswer str:  o long for\n",
      "\tAnswer str2:  ['o', ' ', 'l', 'o', 'n', 'g', ' ', 'f', 'o', 'r']\n",
      "\tPrediction str:  nlond tor \n",
      "\tAnswer str:   long for \n",
      "\tAnswer str2:  [' ', 'l', 'o', 'n', 'g', ' ', 'f', 'o', 'r', ' ']\n",
      "\tPrediction str:  tend tor t\n",
      "\tAnswer str:  long for t\n",
      "\tAnswer str2:  ['l', 'o', 'n', 'g', ' ', 'f', 'o', 'r', ' ', 't']\n",
      "\tPrediction str:  end tor th\n",
      "\tAnswer str:  ong for th\n",
      "\tAnswer str2:  ['o', 'n', 'g', ' ', 'f', 'o', 'r', ' ', 't', 'h']\n",
      "\tPrediction str:  nd tor the\n",
      "\tAnswer str:  ng for the\n",
      "\tAnswer str2:  ['n', 'g', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e']\n",
      "\tPrediction str:  d tor them\n",
      "\tAnswer str:  g for the \n",
      "\tAnswer str2:  ['g', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ']\n",
      "\tPrediction str:  etor toeme\n",
      "\tAnswer str:   for the e\n",
      "\tAnswer str2:  [' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'e']\n",
      "\tPrediction str:  tor toemen\n",
      "\tAnswer str:  for the en\n",
      "\tAnswer str2:  ['f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'e', 'n']\n",
      "\tPrediction str:  o  toemend\n",
      "\tAnswer str:  or the end\n",
      "\tAnswer str2:  ['o', 'r', ' ', 't', 'h', 'e', ' ', 'e', 'n', 'd']\n",
      "\tPrediction str:  n themend \n",
      "\tAnswer str:  r the endl\n",
      "\tAnswer str2:  ['r', ' ', 't', 'h', 'e', ' ', 'e', 'n', 'd', 'l']\n",
      "\tPrediction str:   themend e\n",
      "\tAnswer str:   the endle\n",
      "\tAnswer str2:  [' ', 't', 'h', 'e', ' ', 'e', 'n', 'd', 'l', 'e']\n",
      "\tPrediction str:  toerend es\n",
      "\tAnswer str:  the endles\n",
      "\tAnswer str2:  ['t', 'h', 'e', ' ', 'e', 'n', 'd', 'l', 'e', 's']\n",
      "\tPrediction str:   erend ess\n",
      "\tAnswer str:  he endless\n",
      "\tAnswer str2:  ['h', 'e', ' ', 'e', 'n', 'd', 'l', 'e', 's', 's']\n",
      "\tPrediction str:  emend ess \n",
      "\tAnswer str:  e endless \n",
      "\tAnswer str2:  ['e', ' ', 'e', 'n', 'd', 'l', 'e', 's', 's', ' ']\n",
      "\tPrediction str:  mtsd ess i\n",
      "\tAnswer str:   endless i\n",
      "\tAnswer str2:  [' ', 'e', 'n', 'd', 'l', 'e', 's', 's', ' ', 'i']\n",
      "\tPrediction str:  tod ess im\n",
      "\tAnswer str:  endless im\n",
      "\tAnswer str2:  ['e', 'n', 'd', 'l', 'e', 's', 's', ' ', 'i', 'm']\n",
      "\tPrediction str:  ms ess imm\n",
      "\tAnswer str:  ndless imm\n",
      "\tAnswer str2:  ['n', 'd', 'l', 'e', 's', 's', ' ', 'i', 'm', 'm']\n",
      "\tPrediction str:  d ess imme\n",
      "\tAnswer str:  dless imme\n",
      "\tAnswer str2:  ['d', 'l', 'e', 's', 's', ' ', 'i', 'm', 'm', 'e']\n",
      "\tPrediction str:   ess immen\n",
      "\tAnswer str:  less immen\n",
      "\tAnswer str2:  ['l', 'e', 's', 's', ' ', 'i', 'm', 'm', 'e', 'n']\n",
      "\tPrediction str:  ess immens\n",
      "\tAnswer str:  ess immens\n",
      "\tAnswer str2:  ['e', 's', 's', ' ', 'i', 'm', 'm', 'e', 'n', 's']\n",
      "\tPrediction str:  m  immens \n",
      "\tAnswer str:  ss immensi\n",
      "\tAnswer str2:  ['s', 's', ' ', 'i', 'm', 'm', 'e', 'n', 's', 'i']\n",
      "\tPrediction str:  iiimmens t\n",
      "\tAnswer str:  s immensit\n",
      "\tAnswer str2:  ['s', ' ', 'i', 'm', 'm', 'e', 'n', 's', 'i', 't']\n",
      "\tPrediction str:  iimmens t \n",
      "\tAnswer str:   immensity\n",
      "\tAnswer str2:  [' ', 'i', 'm', 'm', 'e', 'n', 's', 'i', 't', 'y']\n",
      "\tPrediction str:  tmmens t  \n",
      "\tAnswer str:  immensity \n",
      "\tAnswer str2:  ['i', 'm', 'm', 'e', 'n', 's', 'i', 't', 'y', ' ']\n",
      "\tPrediction str:  tmens t   \n",
      "\tAnswer str:  mmensity o\n",
      "\tAnswer str2:  ['m', 'm', 'e', 'n', 's', 'i', 't', 'y', ' ', 'o']\n",
      "\tPrediction str:  e  s t    \n",
      "\tAnswer str:  mensity of\n",
      "\tAnswer str2:  ['m', 'e', 'n', 's', 'i', 't', 'y', ' ', 'o', 'f']\n",
      "\tPrediction str:  e s t     \n",
      "\tAnswer str:  ensity of \n",
      "\tAnswer str2:  ['e', 'n', 's', 'i', 't', 'y', ' ', 'o', 'f', ' ']\n",
      "\tPrediction str:  ms t     t\n",
      "\tAnswer str:  nsity of t\n",
      "\tAnswer str2:  ['n', 's', 'i', 't', 'y', ' ', 'o', 'f', ' ', 't']\n",
      "\tPrediction str:  d t     th\n",
      "\tAnswer str:  sity of th\n",
      "\tAnswer str2:  ['s', 'i', 't', 'y', ' ', 'o', 'f', ' ', 't', 'h']\n",
      "\tPrediction str:  ig     the\n",
      "\tAnswer str:  ity of the\n",
      "\tAnswer str2:  ['i', 't', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e']\n",
      "\tPrediction str:  t     the \n",
      "\tAnswer str:  ty of the \n",
      "\tAnswer str2:  ['t', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ']\n",
      "\tPrediction str:    eo the s\n",
      "\tAnswer str:  y of the s\n",
      "\tAnswer str2:  ['y', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 's']\n",
      "\tPrediction str:   uo the se\n",
      "\tAnswer str:   of the se\n",
      "\tAnswer str2:  [' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 's', 'e']\n",
      "\tPrediction str:  to the sea\n",
      "\tAnswer str:  of the sea\n",
      "\tAnswer str2:  ['o', 'f', ' ', 't', 'h', 'e', ' ', 's', 'e', 'a']\n",
      "\tPrediction str:  n the seas\n",
      "\tAnswer str:  f the sea.\n",
      "\tAnswer str2:  ['f', ' ', 't', 'h', 'e', ' ', 's', 'e', 'a', '.']\n",
      "Answer_full:  f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "Result_full:  t you  ant to t  ld a shi,, dondt aru  u   eoeee together to eolle t  ood and dondt assign them tosss and dor ,  ut rather tea h them to lond tor themend ess immens t     the seas\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "char_set = list(set(sentence)) # 유니크한 문자열을 리스트로 넣는다.\n",
    "char_dic = {w: i for i, w in enumerate(char_set)} # 리스트 문자열을 index로 매칭되는 딕셔너리를 만든다.\n",
    "\n",
    "\n",
    "# hyper parameters\n",
    "data_dim = len(char_set)\n",
    "rnn_hidden_size = len(char_set)\n",
    "num_classes = len(char_set)\n",
    "sequence_length = 10 # Any value\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i+sequence_length]\n",
    "    y_str = sentence[i+1:i+sequence_length+1]\n",
    "    # print( '{0:03d} {1}->{2}'.format(i, x_str, y_str) )\n",
    "    \n",
    "    x = [char_dic[c] for c in x_str]\n",
    "    y = [char_dic[c] for c in y_str]\n",
    "    \n",
    "    x_data.append(x)\n",
    "    y_data.append(y)\n",
    "          \n",
    "\n",
    "batch_size = len(x_data)\n",
    "        \n",
    "# x_data = [sample_idx[:-1]] # X data sample (0 ~ n-1) hello:hell\n",
    "# y_data = [sample_idx[1:]]  # Y label sample ( 1~ n) hello: ello\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "X_one_hot = tf.one_hot(X, num_classes)\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size,  tf.float32)\n",
    "outputs, _state = tf.nn.dynamic_rnn(cell, X_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)\n",
    "\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "sess =  tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(1000):\n",
    "    loss_, _ = sess.run([loss, train], feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    print(i, \"loss: \", loss_)\n",
    "\n",
    "result = sess.run(prediction, feed_dict={X:x_data})\n",
    "\n",
    "answer_full = \"\"\n",
    "result_full = \"\"\n",
    "i = 0\n",
    "for (y, r) in zip(y_data, result):\n",
    "\n",
    "    # print(\"prediction: \", r, \"true Y: \", y)\n",
    "    # print char using dic\n",
    "    result_str = [char_set[c] for c in np.squeeze(r)]\n",
    "    print(\"\\tPrediction str: \", ''.join(result_str))\n",
    "\n",
    "    answer_str = [char_set[c] for c in np.squeeze(y)]\n",
    "    print(\"\\tAnswer str: \", ''.join(answer_str))\n",
    "    print(\"\\tAnswer str2: \",  answer_str)\n",
    "\n",
    "    if i==0:\n",
    "        answer_full = \"{}{}\".format(answer_full, ''.join(answer_str))\n",
    "    else:\n",
    "        answer_full = \"{}{}\".format(answer_full, answer_str[-1])\n",
    "\n",
    "    if i==0:\n",
    "        result_full = \"{}{}\".format(result_full, ''.join(result_str))\n",
    "    else:\n",
    "        result_full = \"{}{}\".format(result_full, result_str[-1])\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "print(\"Answer_full: \", answer_full)\n",
    "print(\"Result_full: \", result_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tc",
   "language": "python",
   "name": "tc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
