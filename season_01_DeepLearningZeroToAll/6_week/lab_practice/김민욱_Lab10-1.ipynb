{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab10-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Source1 - Softmax MNIST(Simple Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 4.769888724\n",
      "Epoch: 0002 cost = 1.615334014\n",
      "Epoch: 0003 cost = 1.087040412\n",
      "Epoch: 0004 cost = 0.869196778\n",
      "Epoch: 0005 cost = 0.744912507\n",
      "Epoch: 0006 cost = 0.663591566\n",
      "Epoch: 0007 cost = 0.605157003\n",
      "Epoch: 0008 cost = 0.561292389\n",
      "Epoch: 0009 cost = 0.526674109\n",
      "Epoch: 0010 cost = 0.498675998\n",
      "Epoch: 0011 cost = 0.475497245\n",
      "Epoch: 0012 cost = 0.456280063\n",
      "Epoch: 0013 cost = 0.439509648\n",
      "Epoch: 0014 cost = 0.425232265\n",
      "Epoch: 0015 cost = 0.412625212\n",
      "Accuracy:  0.896\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([1, 10]))\n",
    "\n",
    "# define Hypothesis\n",
    "logits = tf.matmul(X, W)+b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# define cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# define gradient\n",
    "gradient = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# calc accuracy\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "answer = tf.argmax(Y, 1)\n",
    "accuracy = tf.reduce_mean( tf.cast( tf.equal(prediction, answer), dtype=tf.float32)) \n",
    "\n",
    "# tensor board setting\n",
    "tf.summary.scalar(\"cost\", cost)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./log/MNIST/SimpleAdam\")\n",
    "\n",
    "# init network\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# go implement\n",
    "traing_epoch = 15\n",
    "batch_size = 100\n",
    "\n",
    "total_recur = 0\n",
    "\n",
    "for epoch in range(traing_epoch):\n",
    "    avg_cost = 0.0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for batch in range(total_batch):\n",
    "        batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "        cost_, summary_, _ = sess.run([cost, summary, gradient], feed_dict={X: batch_X, Y:batch_Y})\n",
    "        avg_cost += cost_/total_batch\n",
    "        writer.add_summary(summary_, global_step=total_recur)\n",
    "        total_recur = total_recur + 1\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "# implement accuracy\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: mnist.test.images, Y:mnist.test.labels} ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source2 - Softmax MNIST(Deep Relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 153.359758790\n",
      "Epoch: 0002 cost = 40.508465873\n",
      "Epoch: 0003 cost = 25.561788379\n",
      "Epoch: 0004 cost = 17.753726211\n",
      "Epoch: 0005 cost = 12.808236501\n",
      "Epoch: 0006 cost = 9.587044900\n",
      "Epoch: 0007 cost = 7.178024372\n",
      "Epoch: 0008 cost = 5.475538693\n",
      "Epoch: 0009 cost = 4.097949056\n",
      "Epoch: 0010 cost = 3.088043055\n",
      "Epoch: 0011 cost = 2.236451875\n",
      "Epoch: 0012 cost = 1.785966832\n",
      "Epoch: 0013 cost = 1.433731425\n",
      "Epoch: 0014 cost = 1.122132656\n",
      "Epoch: 0015 cost = 0.865378940\n",
      "Accuracy:  0.9453\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([1, 10]))\n",
    "\n",
    "# define Hypothesis\n",
    "logits = tf.matmul(L2, W3)+b3\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# define cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# define gradient\n",
    "gradient = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# calc accuracy\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "answer = tf.argmax(Y, 1)\n",
    "accuracy = tf.reduce_mean( tf.cast( tf.equal(prediction, answer), dtype=tf.float32)) \n",
    "\n",
    "# tensor board setting\n",
    "tf.summary.scalar(\"cost\", cost)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./log/MNIST/DeepRelu\")\n",
    "\n",
    "# init network\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# go implement\n",
    "traing_epoch = 15\n",
    "batch_size = 100\n",
    "\n",
    "total_recur = 0\n",
    "\n",
    "for epoch in range(traing_epoch):\n",
    "    avg_cost = 0.0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for batch in range(total_batch):\n",
    "        batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "        cost_, summary_, _ = sess.run([cost, summary, gradient], feed_dict={X: batch_X, Y:batch_Y})\n",
    "        avg_cost += cost_/total_batch\n",
    "        writer.add_summary(summary_, global_step=total_recur)\n",
    "        total_recur = total_recur + 1\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "# implement accuracy\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: mnist.test.images, Y:mnist.test.labels} ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source3 - Softmax MNIST(Xavier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.308126761\n",
      "Epoch: 0002 cost = 0.114551528\n",
      "Epoch: 0003 cost = 0.076035305\n",
      "Epoch: 0004 cost = 0.052281257\n",
      "Epoch: 0005 cost = 0.041228520\n",
      "Epoch: 0006 cost = 0.030583160\n",
      "Epoch: 0007 cost = 0.022635838\n",
      "Epoch: 0008 cost = 0.019903191\n",
      "Epoch: 0009 cost = 0.018020939\n",
      "Epoch: 0010 cost = 0.013173883\n",
      "Epoch: 0011 cost = 0.013564116\n",
      "Epoch: 0012 cost = 0.011045476\n",
      "Epoch: 0013 cost = 0.011434260\n",
      "Epoch: 0014 cost = 0.010925517\n",
      "Epoch: 0015 cost = 0.008962857\n",
      "Accuracy:  0.9781\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1)+b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2)+b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([1, 10]))\n",
    "\n",
    "# define Hypothesis\n",
    "logits = tf.matmul(L2, W3)+b3\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# define cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# define gradient\n",
    "gradient = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# calc accuracy\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "answer = tf.argmax(Y, 1)\n",
    "accuracy = tf.reduce_mean( tf.cast( tf.equal(prediction, answer), dtype=tf.float32)) \n",
    "\n",
    "# tensor board setting\n",
    "tf.summary.scalar(\"cost\", cost)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./log/MNIST/Xavier\")\n",
    "\n",
    "# init network\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# go implement\n",
    "traing_epoch = 15\n",
    "batch_size = 100\n",
    "\n",
    "total_recur = 0\n",
    "\n",
    "for epoch in range(traing_epoch):\n",
    "    avg_cost = 0.0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for batch in range(total_batch):\n",
    "        batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "        cost_, summary_, _ = sess.run([cost, summary, gradient], feed_dict={X: batch_X, Y:batch_Y})\n",
    "        avg_cost += cost_/total_batch\n",
    "        writer.add_summary(summary_, global_step=total_recur)\n",
    "        total_recur = total_recur + 1\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "# implement accuracy\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: mnist.test.images, Y:mnist.test.labels} ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source4 - Softmax MNIST(Deep Xavier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.306926312\n",
      "Epoch: 0002 cost = 0.107997548\n",
      "Epoch: 0003 cost = 0.074566406\n",
      "Epoch: 0004 cost = 0.056467985\n",
      "Epoch: 0005 cost = 0.044490910\n",
      "Epoch: 0006 cost = 0.038901835\n",
      "Epoch: 0007 cost = 0.031867029\n",
      "Epoch: 0008 cost = 0.026287721\n",
      "Epoch: 0009 cost = 0.021127776\n",
      "Epoch: 0010 cost = 0.021204374\n",
      "Epoch: 0011 cost = 0.018046587\n",
      "Epoch: 0012 cost = 0.018727395\n",
      "Epoch: 0013 cost = 0.017877394\n",
      "Epoch: 0014 cost = 0.013491482\n",
      "Epoch: 0015 cost = 0.014224143\n",
      "Accuracy:  0.977\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1)+b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2)+b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3)+b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4)+b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([1, 10]))\n",
    "\n",
    "# define Hypothesis\n",
    "logits = tf.matmul(L4, W5)+b5\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# define cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# define gradient\n",
    "gradient = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# calc accuracy\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "answer = tf.argmax(Y, 1)\n",
    "accuracy = tf.reduce_mean( tf.cast( tf.equal(prediction, answer), dtype=tf.float32)) \n",
    "\n",
    "# tensor board setting\n",
    "tf.summary.scalar(\"cost\", cost)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./log/MNIST/DeepXavier\")\n",
    "\n",
    "# init network\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# go implement\n",
    "traing_epoch = 15\n",
    "batch_size = 100\n",
    "\n",
    "total_recur = 0\n",
    "\n",
    "for epoch in range(traing_epoch):\n",
    "    avg_cost = 0.0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for batch in range(total_batch):\n",
    "        batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "        cost_, summary_, _ = sess.run([cost, summary, gradient], feed_dict={X: batch_X, Y:batch_Y})\n",
    "        avg_cost += cost_/total_batch\n",
    "        writer.add_summary(summary_, global_step=total_recur)\n",
    "        total_recur = total_recur + 1\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "# implement accuracy\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: mnist.test.images, Y:mnist.test.labels} ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source5 - Softmax MNIST(Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.566080621\n",
      "Epoch: 0002 cost = 0.212592788\n",
      "Epoch: 0003 cost = 0.162051453\n",
      "Epoch: 0004 cost = 0.136791808\n",
      "Epoch: 0005 cost = 0.116340544\n",
      "Epoch: 0006 cost = 0.108492268\n",
      "Epoch: 0007 cost = 0.098913190\n",
      "Epoch: 0008 cost = 0.088963184\n",
      "Epoch: 0009 cost = 0.085316288\n",
      "Epoch: 0010 cost = 0.078933604\n",
      "Epoch: 0011 cost = 0.074244852\n",
      "Epoch: 0012 cost = 0.068430431\n",
      "Epoch: 0013 cost = 0.065250638\n",
      "Epoch: 0014 cost = 0.065891355\n",
      "Epoch: 0015 cost = 0.061583157\n",
      "Accuracy:  0.9783\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1)+b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2)+b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3)+b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([1, 256]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4)+b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([1, 10]))\n",
    "\n",
    "# define Hypothesis\n",
    "logits = tf.matmul(L4, W5)+b5\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# define cost\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "# define gradient\n",
    "gradient = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# calc accuracy\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "answer = tf.argmax(Y, 1)\n",
    "accuracy = tf.reduce_mean( tf.cast( tf.equal(prediction, answer), dtype=tf.float32)) \n",
    "\n",
    "# tensor board setting\n",
    "tf.summary.scalar(\"cost\", cost)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./log/MNIST/Dropout\")\n",
    "\n",
    "# init network\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# go implement\n",
    "traing_epoch = 15\n",
    "batch_size = 100\n",
    "\n",
    "total_recur = 0\n",
    "\n",
    "for epoch in range(traing_epoch):\n",
    "    avg_cost = 0.0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for batch in range(total_batch):\n",
    "        batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "        cost_, summary_, _ = sess.run([cost, summary, gradient], feed_dict={X: batch_X, Y:batch_Y, keep_prob: 0.7})\n",
    "        avg_cost += cost_/total_batch\n",
    "        writer.add_summary(summary_, global_step=total_recur)\n",
    "        total_recur = total_recur + 1\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "# implement accuracy\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: mnist.test.images, Y:mnist.test.labels, keep_prob: 1.0} ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensorboard](./김민욱_summary_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensorboard](./김민욱_summary_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tc",
   "language": "python",
   "name": "tc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
